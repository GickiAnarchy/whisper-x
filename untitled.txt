import whisperx
import gc

device = "cuda"
        batch_size = 16 # reduce if low on GPU memory
        compute_type = "float16" # change to "int8" if low on GPU memory (may reduce accuracy)

        # 1. Load audio
        audio_file = "your_audio_file.mp3" # Replace with your uploaded file name
        audio = whisperx.load_audio(audio_file)

        # 2. Load model and transcribe
        model = whisperx.load_model("large-v2", device=device, compute_type=compute_type)
        result = model.transcribe(audio, batch_size=batch_size)

        # delete model if low on GPU memory
        # import gc; gc.collect(); torch.cuda.empty_cache(); del model

        # 3. Align whisper output
        model_a, metadata = whisperx.load_align_model(language_code=result["language"], device=device)
        result = whisperx.align(result["segments"], model_a, audio, metadata, device)

        # delete model if low on GPU memory
        # import gc; gc.collect(); torch.cuda.empty_cache(); del model_a